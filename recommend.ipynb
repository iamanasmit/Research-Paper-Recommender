{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import csv_loader\n",
    "loader=csv_loader.CSVLoader(\"arxiv_data_210930-054931.csv\", encoding='utf8')\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms: ['cs.LG']\n",
      "titles: Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
      "abstracts: Graph neural networks (GNNs) have been widely used to learn vector\n",
      "representation of graph-structured data and achieved better task performance\n",
      "than conventional methods. The foundation of GNNs is the message passing\n",
      "procedure, which propagates the information in a node to its neighbors. Since\n",
      "this procedure proceeds one step per layer, the range of the information\n",
      "propagation among nodes is small in the lower layers, and it expands toward the\n",
      "higher layers. Therefore, a GNN model has to be deep enough to capture global\n",
      "structural information in a graph. On the other hand, it is known that deep GNN\n",
      "models suffer from performance degradation because they lose nodes' local\n",
      "information, which would be essential for good model performance, through many\n",
      "message passing steps. In this study, we propose multi-level attention pooling\n",
      "(MLAP) for graph-level classification tasks, which can adapt to both local and\n",
      "global structural information in a graph. It has an attention pooling layer for\n",
      "each message passing step and computes the final graph representation by\n",
      "unifying the layer-wise graph representations. The MLAP architecture allows\n",
      "models to utilize the structural information of graphs with multiple levels of\n",
      "localities because it preserves layer-wise information before losing them due\n",
      "to oversmoothing. Results of our experiments show that the MLAP architecture\n",
      "improves the graph classification performance compared to the baseline\n",
      "architectures. In addition, analyses on the layer-wise graph representations\n",
      "suggest that aggregating information from multiple levels of localities indeed\n",
      "has the potential to improve the discriminability of learned graph\n",
      "representations.\n",
      "{'source': 'arxiv_data_210930-054931.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)\n",
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anasm\\AppData\\Local\\Temp\\ipykernel_16116\\927919671.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\anasm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\anasm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anasm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db=FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('vectorstore.pkl', 'wb') as f:\n",
    "    pickle.dump(db, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'arxiv_data_210930-054931.csv', 'row': 33669}, page_content=\"terms: ['cs.LG', 'stat.ML']\\ntitles: An Attentive Survey of Attention Models\\nabstracts: Attention Model has now become an important concept in neural networks that\\nhas been researched within diverse application domains. This survey provides a\\nstructured and comprehensive overview of the developments in modeling\\nattention. In particular, we propose a taxonomy which groups existing\\ntechniques into coherent categories. We review salient neural architectures in\\nwhich attention has been incorporated, and discuss applications in which\\nmodeling attention has shown a significant impact. We also describe how\\nattention has been used to improve the interpretability of neural networks.\\nFinally, we discuss some future research directions in attention. We hope this\\nsurvey will provide a succinct introduction to attention models and guide\\npractitioners while developing approaches for their applications.\"),\n",
       " Document(metadata={'source': 'arxiv_data_210930-054931.csv', 'row': 38666}, page_content=\"terms: ['cs.LG', 'cs.CL', 'stat.ML']\\ntitles: Copy this Sentence\\nabstracts: Attention is an operation that selects some largest element from some set,\\nwhere the notion of largest is defined elsewhere. Applying this operation to\\nsequence to sequence mapping results in significant improvements to the task at\\nhand. In this paper we provide the mathematical definition of attention and\\nexamine its application to sequence to sequence models. We highlight the exact\\ncorrespondences between machine learning implementations of attention and our\\nmathematical definition. We provide clear evidence of effectiveness of\\nattention mechanisms evaluating models with varying degrees of attention on a\\nvery simple task: copying a sentence. We find that models that make greater use\\nof attention perform much better on sequence to sequence mapping tasks,\\nconverge faster and are more stable.\"),\n",
       " Document(metadata={'source': 'arxiv_data_210930-054931.csv', 'row': 1248}, page_content=\"terms: ['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']\\ntitles: Area Attention\\nabstracts: Existing attention mechanisms are trained to attend to individual items in a\\ncollection (the memory) with a predefined, fixed granularity, e.g., a word\\ntoken or an image grid. We propose area attention: a way to attend to areas in\\nthe memory, where each area contains a group of items that are structurally\\nadjacent, e.g., spatially for a 2D memory such as images, or temporally for a\\n1D memory such as natural language sentences. Importantly, the shape and the\\nsize of an area are dynamically determined via learning, which enables a model\\nto attend to information with varying granularity. Area attention can easily\\nwork with existing model architectures such as multi-head attention for\\nsimultaneously attending to multiple areas in the memory. We evaluate area\\nattention on two tasks: neural machine translation (both character and\\ntoken-level) and image captioning, and improve upon strong (state-of-the-art)\\nbaselines in all the cases. These improvements are obtainable with a basic form\\nof area attention that is parameter free.\"),\n",
       " Document(metadata={'source': 'arxiv_data_210930-054931.csv', 'row': 37056}, page_content=\"terms: ['cs.LG', 'cs.AI', 'cs.CL', 'stat.ML']\\ntitles: Area Attention\\nabstracts: Existing attention mechanisms are trained to attend to individual items in a\\ncollection (the memory) with a predefined, fixed granularity, e.g., a word\\ntoken or an image grid. We propose area attention: a way to attend to areas in\\nthe memory, where each area contains a group of items that are structurally\\nadjacent, e.g., spatially for a 2D memory such as images, or temporally for a\\n1D memory such as natural language sentences. Importantly, the shape and the\\nsize of an area are dynamically determined via learning, which enables a model\\nto attend to information with varying granularity. Area attention can easily\\nwork with existing model architectures such as multi-head attention for\\nsimultaneously attending to multiple areas in the memory. We evaluate area\\nattention on two tasks: neural machine translation (both character and\\ntoken-level) and image captioning, and improve upon strong (state-of-the-art)\\nbaselines in all the cases. These improvements are obtainable with a basic form\\nof area attention that is parameter free.\"),\n",
       " Document(metadata={'source': 'arxiv_data_210930-054931.csv', 'row': 34105}, page_content=\"terms: ['cs.CV', 'cs.LG']\\ntitles: Understanding top-down attention using task-oriented ablation design\\nabstracts: Top-down attention allows neural networks, both artificial and biological, to\\nfocus on the information most relevant for a given task. This is known to\\nenhance performance in visual perception. But it remains unclear how attention\\nbrings about its perceptual boost, especially when it comes to naturalistic\\nsettings like recognising an object in an everyday scene. What aspects of a\\nvisual task does attention help to deal with? We aim to answer this with a\\ncomputational experiment based on a general framework called task-oriented\\nablation design. First we define a broad range of visual tasks and identify six\\nfactors that underlie task variability. Then on each task we compare the\\nperformance of two neural networks, one with top-down attention and one\\nwithout. These comparisons reveal the task-dependence of attention's perceptual\\nboost, giving a clearer idea of the role attention plays. Whereas many existing\\ncognitive accounts link attention to stimulus-level variables, such as visual\\nclutter and object scale, we find greater explanatory power in system-level\\nvariables that capture the interaction between the model, the distribution of\\ntraining data and the task format. This finding suggests a shift in how\\nattention is studied could be fruitful. We make publicly available our code and\\nresults, along with statistics relevant to ImageNet-based experiments beyond\\nthis one. Our contribution serves to support the development of more human-like\\nvision models and the design of more informative machine-learning experiments.\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"Attention is all you need\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
